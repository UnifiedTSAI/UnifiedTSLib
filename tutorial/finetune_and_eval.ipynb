{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimerMixerPP Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval -> eval_mixerpp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "from Modeling.datasets.benchmark_dataset import ChannelEvalDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 eval_mixerpp.py\n",
    "\n",
    "`UnifiedTS` is a utility class for loading and performing inference with time series forecasting models. By initializing this class, you can easily load a model onto a specified device and perform batch predictions, making it suitable for model evaluation or real-world inference scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedTS:\n",
    "    def __init__(self, model_path, device, context_length, prediction_length, **kwargs):\n",
    "        from Modeling.models.modeling_UnifiedTS import UnifiedTSForPrediction,UnifiedTSConfig\n",
    "        self.model = UnifiedTSForPrediction.from_pretrained(\n",
    "            model_path,  \n",
    "            # device_map=\"auto\",\n",
    "            torch_dtype=\"auto\"\n",
    "        )\n",
    "        self.device = device\n",
    "        self.model = self.model.to(self.device) \n",
    "        self.prediction_length = prediction_length\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, batch):\n",
    "       \n",
    "        inputs = batch['inputs'].to(self.device)\n",
    "        labels = batch['labels'].to(self.device)\n",
    "        outputs = self.model(input_ids=inputs, labels=labels)\n",
    "\n",
    "        preds = outputs.logits if isinstance(outputs.logits, list) else [outputs.logits]\n",
    "        labels_slices = [labels[:, :pl] for pl in self.model.config.pred_len]\n",
    "        return preds, labels_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation Workflow\n",
    "The `evaluate` function provides a complete workflow for evaluating a time series forecasting model on a benchmark dataset. The process includes model loading, data preparation, batch inference, and metric calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Initialize Model and Device\n",
    "First, set up the device (GPU or CPU) and load the forecasting model using the `UnifiedTS` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args):\n",
    "    batch_size = args.batch_size\n",
    "    context_length = args.context_length\n",
    "    prediction_lengths = [96, 192, 336, 720]  \n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model = UnifiedTS(\n",
    "        args.model_path,\n",
    "        device,\n",
    "        context_length=context_length,\n",
    "        prediction_length=prediction_lengths,  \n",
    "        channel_mixing=args.channel_mixing\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Prepare Evaluation Metrics\n",
    "Create a dictionary to store MSE and MAE metrics for each prediction length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_metrics = {pl: {'mse': 0.0, 'mae': 0.0, 'count': 0} for pl in prediction_lengths}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Loop Over Prediction Lengths and Evaluate\n",
    "For each prediction length, load the evaluation dataset, create a DataLoader, and compute metrics batch by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pl in prediction_lengths:\n",
    "        dataset = ChannelEvalDataset(\n",
    "            args.data,\n",
    "            context_length=context_length,\n",
    "            prediction_length=pl\n",
    "        )\n",
    "        test_dl = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            prefetch_factor=2,\n",
    "            drop_last=False\n",
    "        )\n",
    "        local_mse = 0.0\n",
    "        local_mae = 0.0\n",
    "        local_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_dl, desc=f\"Testing pl={pl}\"):\n",
    "                all_preds, all_labels = model.predict(batch)\n",
    "                for pred, label, pred_len in zip(all_preds, all_labels, model.model.config.pred_len):\n",
    "                    if pred_len == pl:\n",
    "                        pred = pred.squeeze(-1)\n",
    "                        label = label.squeeze(-1)\n",
    "                        local_mse += torch.sum((pred - label) ** 2).item()\n",
    "                        local_mae += torch.sum(torch.abs(pred - label)).item()\n",
    "                        local_count += pred.numel()\n",
    "                        break\n",
    "        if local_count > 0:\n",
    "            global_metrics[pl]['mse'] = local_mse / local_count\n",
    "            global_metrics[pl]['mae'] = local_mae / local_count\n",
    "            global_metrics[pl]['count'] = local_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Print and Log the Evaluation Results\n",
    "After evaluation, print and log the final results for each prediction length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    result = {\n",
    "        'model': args.model_path,\n",
    "        'data': args.data,\n",
    "        'context_length': context_length,\n",
    "        'metrics': {}\n",
    "    }\n",
    "\n",
    "    for pl in prediction_lengths:\n",
    "        if global_metrics[pl]['count'] > 0:\n",
    "            result['metrics'].update({\n",
    "                f'mse_{pl}': global_metrics[pl]['mse'],\n",
    "                f'mae_{pl}': global_metrics[pl]['mae']\n",
    "            })\n",
    "\n",
    "    logging.info(json.dumps(result, indent=2))\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for pl in prediction_lengths:\n",
    "        print(f\"PredLen {pl}: MSE={global_metrics[pl]['mse']:.4f}, MAE={global_metrics[pl]['mae']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Evaluation Script\n",
    "You can evaluate your trained model using the `eval_mixerPP.py` script. Specify the dataset path, enable channel mixing, set the batch size, and provide the model path as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python eval_mixerPP.py \\\n",
    "    -d data/test/data_etth1_train/ \\\n",
    "    --channel_mixing True \\\n",
    "    --batch_size 1024 \\\n",
    "    --model_path logs/timemixerpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning Process\n",
    "This project supports fine-tuning a pretrained time series forecasting model on these datasets . The fine-tuning workflow is managed by the `main.py` script, which configures the training process and launches model training via the `Runner` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Fine-tuning Workflow Overview\n",
    "- Argument Parsing:\n",
    "`main.py` uses `argparse` to parse a wide range of training and model parameters, such as model path, data paths, batch size, learning rates, and more.\n",
    "- Runner Initialization:\n",
    "The script initializes a `Runner` object, which manages the training process.\n",
    "- Training Start:\n",
    "The `train_model` method is called to start fine-tuning, using the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py (simplified)\n",
    "from Modeling.runner import Runner\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # ... (argument definitions) ...\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    runner = Runner(\n",
    "        model_path=args.model_path,\n",
    "        output_path=args.output_path,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    runner.train_model(\n",
    "        model_name=args.model_name,\n",
    "        from_scratch=args.from_scratch,\n",
    "        context_length=args.context_length,\n",
    "        prediction_length=args.prediction_length,\n",
    "        # ... (other training parameters) ...\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Distributed Training Launcher\n",
    "For efficient training on multiple GPUs, use the `torch_dist_run.py` script. This script automatically launches distributed training using `torchrun` if GPUs are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Example Fine-tuning Command\n",
    "You can start fine-tuning your model with a command like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python torch_dist_run.py main.py \\\n",
    "    --micro_batch_size 1 \\\n",
    "    --global_batch_size 8 \\\n",
    "    --channel_mixing True \\\n",
    "    -o logs/timemixerpp_traffic_finetune \\\n",
    "    -d data/train/data_electricity_train/ \\\n",
    "    -m /opt/tiger/UnifiedTSLib/logs/timemixerpp_1 \\\n",
    "    --val_data_path data/val/data_electricity_validation/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `main.py`: The main training script.\n",
    "- `--micro_batch_size`: Per-GPU batch size.\n",
    "- `--global_batch_size`: Total batch size across all GPUs.\n",
    "- `--channel_mixing True`: Enable channel mixing.\n",
    "- `-o`: Output directory for logs and checkpoints.\n",
    "- `-d`: Path to the training dataset.\n",
    "- `-m`: Path to the pretrained model to be fine-tuned.\n",
    "- `--val_data_path`: Path to the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "fileId": "0b9d2ec2-8dd1-4364-8fa9-bcd718ea5831",
  "filePath": "/opt/tiger/UnifiedTSLib/tutorial/finetune_and_eval.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
